{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eae71a67-a21d-4d71-ba6b-dcc8990b0673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CLIP import CLIPModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0f1d9f-26fe-4a3b-befe-d01a91441fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['image_encoder1.model.stages.0.0.convs.0.conv.weight', 'image_encoder1.model.stages.0.0.convs.0.conv.bias', 'image_encoder1.model.stages.0.0.convs.0.norm.weight', 'image_encoder1.model.stages.0.0.convs.0.norm.bias', 'image_encoder1.model.stages.0.0.convs.0.all_modules.0.weight', 'image_encoder1.model.stages.0.0.convs.0.all_modules.0.bias', 'image_encoder1.model.stages.0.0.convs.0.all_modules.1.weight', 'image_encoder1.model.stages.0.0.convs.0.all_modules.1.bias', 'image_encoder1.model.stages.0.0.convs.1.conv.weight', 'image_encoder1.model.stages.0.0.convs.1.conv.bias', 'image_encoder1.model.stages.0.0.convs.1.norm.weight', 'image_encoder1.model.stages.0.0.convs.1.norm.bias', 'image_encoder1.model.stages.0.0.convs.1.all_modules.0.weight', 'image_encoder1.model.stages.0.0.convs.1.all_modules.0.bias', 'image_encoder1.model.stages.0.0.convs.1.all_modules.1.weight', 'image_encoder1.model.stages.0.0.convs.1.all_modules.1.bias', 'image_encoder1.model.stages.1.0.convs.0.conv.weight', 'image_encoder1.model.stages.1.0.convs.0.conv.bias', 'image_encoder1.model.stages.1.0.convs.0.norm.weight', 'image_encoder1.model.stages.1.0.convs.0.norm.bias', 'image_encoder1.model.stages.1.0.convs.0.all_modules.0.weight', 'image_encoder1.model.stages.1.0.convs.0.all_modules.0.bias', 'image_encoder1.model.stages.1.0.convs.0.all_modules.1.weight', 'image_encoder1.model.stages.1.0.convs.0.all_modules.1.bias', 'image_encoder1.model.stages.1.0.convs.1.conv.weight', 'image_encoder1.model.stages.1.0.convs.1.conv.bias', 'image_encoder1.model.stages.1.0.convs.1.norm.weight', 'image_encoder1.model.stages.1.0.convs.1.norm.bias', 'image_encoder1.model.stages.1.0.convs.1.all_modules.0.weight', 'image_encoder1.model.stages.1.0.convs.1.all_modules.0.bias', 'image_encoder1.model.stages.1.0.convs.1.all_modules.1.weight', 'image_encoder1.model.stages.1.0.convs.1.all_modules.1.bias', 'image_encoder1.model.stages.2.0.convs.0.conv.weight', 'image_encoder1.model.stages.2.0.convs.0.conv.bias', 'image_encoder1.model.stages.2.0.convs.0.norm.weight', 'image_encoder1.model.stages.2.0.convs.0.norm.bias', 'image_encoder1.model.stages.2.0.convs.0.all_modules.0.weight', 'image_encoder1.model.stages.2.0.convs.0.all_modules.0.bias', 'image_encoder1.model.stages.2.0.convs.0.all_modules.1.weight', 'image_encoder1.model.stages.2.0.convs.0.all_modules.1.bias', 'image_encoder1.model.stages.2.0.convs.1.conv.weight', 'image_encoder1.model.stages.2.0.convs.1.conv.bias', 'image_encoder1.model.stages.2.0.convs.1.norm.weight', 'image_encoder1.model.stages.2.0.convs.1.norm.bias', 'image_encoder1.model.stages.2.0.convs.1.all_modules.0.weight', 'image_encoder1.model.stages.2.0.convs.1.all_modules.0.bias', 'image_encoder1.model.stages.2.0.convs.1.all_modules.1.weight', 'image_encoder1.model.stages.2.0.convs.1.all_modules.1.bias', 'image_encoder1.model.stages.3.0.convs.0.conv.weight', 'image_encoder1.model.stages.3.0.convs.0.conv.bias', 'image_encoder1.model.stages.3.0.convs.0.norm.weight', 'image_encoder1.model.stages.3.0.convs.0.norm.bias', 'image_encoder1.model.stages.3.0.convs.0.all_modules.0.weight', 'image_encoder1.model.stages.3.0.convs.0.all_modules.0.bias', 'image_encoder1.model.stages.3.0.convs.0.all_modules.1.weight', 'image_encoder1.model.stages.3.0.convs.0.all_modules.1.bias', 'image_encoder1.model.stages.3.0.convs.1.conv.weight', 'image_encoder1.model.stages.3.0.convs.1.conv.bias', 'image_encoder1.model.stages.3.0.convs.1.norm.weight', 'image_encoder1.model.stages.3.0.convs.1.norm.bias', 'image_encoder1.model.stages.3.0.convs.1.all_modules.0.weight', 'image_encoder1.model.stages.3.0.convs.1.all_modules.0.bias', 'image_encoder1.model.stages.3.0.convs.1.all_modules.1.weight', 'image_encoder1.model.stages.3.0.convs.1.all_modules.1.bias', 'image_encoder1.model.stages.4.0.convs.0.conv.weight', 'image_encoder1.model.stages.4.0.convs.0.conv.bias', 'image_encoder1.model.stages.4.0.convs.0.norm.weight', 'image_encoder1.model.stages.4.0.convs.0.norm.bias', 'image_encoder1.model.stages.4.0.convs.0.all_modules.0.weight', 'image_encoder1.model.stages.4.0.convs.0.all_modules.0.bias', 'image_encoder1.model.stages.4.0.convs.0.all_modules.1.weight', 'image_encoder1.model.stages.4.0.convs.0.all_modules.1.bias', 'image_encoder1.model.stages.4.0.convs.1.conv.weight', 'image_encoder1.model.stages.4.0.convs.1.conv.bias', 'image_encoder1.model.stages.4.0.convs.1.norm.weight', 'image_encoder1.model.stages.4.0.convs.1.norm.bias', 'image_encoder1.model.stages.4.0.convs.1.all_modules.0.weight', 'image_encoder1.model.stages.4.0.convs.1.all_modules.0.bias', 'image_encoder1.model.stages.4.0.convs.1.all_modules.1.weight', 'image_encoder1.model.stages.4.0.convs.1.all_modules.1.bias', 'image_encoder1.model.stages.5.0.convs.0.conv.weight', 'image_encoder1.model.stages.5.0.convs.0.conv.bias', 'image_encoder1.model.stages.5.0.convs.0.norm.weight', 'image_encoder1.model.stages.5.0.convs.0.norm.bias', 'image_encoder1.model.stages.5.0.convs.0.all_modules.0.weight', 'image_encoder1.model.stages.5.0.convs.0.all_modules.0.bias', 'image_encoder1.model.stages.5.0.convs.0.all_modules.1.weight', 'image_encoder1.model.stages.5.0.convs.0.all_modules.1.bias', 'image_encoder1.model.stages.5.0.convs.1.conv.weight', 'image_encoder1.model.stages.5.0.convs.1.conv.bias', 'image_encoder1.model.stages.5.0.convs.1.norm.weight', 'image_encoder1.model.stages.5.0.convs.1.norm.bias', 'image_encoder1.model.stages.5.0.convs.1.all_modules.0.weight', 'image_encoder1.model.stages.5.0.convs.1.all_modules.0.bias', 'image_encoder1.model.stages.5.0.convs.1.all_modules.1.weight', 'image_encoder1.model.stages.5.0.convs.1.all_modules.1.bias', 'image_projection1.projection.weight', 'image_projection1.projection.bias', 'image_projection1.fc.weight', 'image_projection1.fc.bias', 'image_projection1.layer_norm.weight', 'image_projection1.layer_norm.bias', 'image_encoder2.model.stages.0.0.convs.0.conv.weight', 'image_encoder2.model.stages.0.0.convs.0.conv.bias', 'image_encoder2.model.stages.0.0.convs.0.norm.weight', 'image_encoder2.model.stages.0.0.convs.0.norm.bias', 'image_encoder2.model.stages.0.0.convs.0.all_modules.0.weight', 'image_encoder2.model.stages.0.0.convs.0.all_modules.0.bias', 'image_encoder2.model.stages.0.0.convs.0.all_modules.1.weight', 'image_encoder2.model.stages.0.0.convs.0.all_modules.1.bias', 'image_encoder2.model.stages.0.0.convs.1.conv.weight', 'image_encoder2.model.stages.0.0.convs.1.conv.bias', 'image_encoder2.model.stages.0.0.convs.1.norm.weight', 'image_encoder2.model.stages.0.0.convs.1.norm.bias', 'image_encoder2.model.stages.0.0.convs.1.all_modules.0.weight', 'image_encoder2.model.stages.0.0.convs.1.all_modules.0.bias', 'image_encoder2.model.stages.0.0.convs.1.all_modules.1.weight', 'image_encoder2.model.stages.0.0.convs.1.all_modules.1.bias', 'image_encoder2.model.stages.1.0.convs.0.conv.weight', 'image_encoder2.model.stages.1.0.convs.0.conv.bias', 'image_encoder2.model.stages.1.0.convs.0.norm.weight', 'image_encoder2.model.stages.1.0.convs.0.norm.bias', 'image_encoder2.model.stages.1.0.convs.0.all_modules.0.weight', 'image_encoder2.model.stages.1.0.convs.0.all_modules.0.bias', 'image_encoder2.model.stages.1.0.convs.0.all_modules.1.weight', 'image_encoder2.model.stages.1.0.convs.0.all_modules.1.bias', 'image_encoder2.model.stages.1.0.convs.1.conv.weight', 'image_encoder2.model.stages.1.0.convs.1.conv.bias', 'image_encoder2.model.stages.1.0.convs.1.norm.weight', 'image_encoder2.model.stages.1.0.convs.1.norm.bias', 'image_encoder2.model.stages.1.0.convs.1.all_modules.0.weight', 'image_encoder2.model.stages.1.0.convs.1.all_modules.0.bias', 'image_encoder2.model.stages.1.0.convs.1.all_modules.1.weight', 'image_encoder2.model.stages.1.0.convs.1.all_modules.1.bias', 'image_encoder2.model.stages.2.0.convs.0.conv.weight', 'image_encoder2.model.stages.2.0.convs.0.conv.bias', 'image_encoder2.model.stages.2.0.convs.0.norm.weight', 'image_encoder2.model.stages.2.0.convs.0.norm.bias', 'image_encoder2.model.stages.2.0.convs.0.all_modules.0.weight', 'image_encoder2.model.stages.2.0.convs.0.all_modules.0.bias', 'image_encoder2.model.stages.2.0.convs.0.all_modules.1.weight', 'image_encoder2.model.stages.2.0.convs.0.all_modules.1.bias', 'image_encoder2.model.stages.2.0.convs.1.conv.weight', 'image_encoder2.model.stages.2.0.convs.1.conv.bias', 'image_encoder2.model.stages.2.0.convs.1.norm.weight', 'image_encoder2.model.stages.2.0.convs.1.norm.bias', 'image_encoder2.model.stages.2.0.convs.1.all_modules.0.weight', 'image_encoder2.model.stages.2.0.convs.1.all_modules.0.bias', 'image_encoder2.model.stages.2.0.convs.1.all_modules.1.weight', 'image_encoder2.model.stages.2.0.convs.1.all_modules.1.bias', 'image_encoder2.model.stages.3.0.convs.0.conv.weight', 'image_encoder2.model.stages.3.0.convs.0.conv.bias', 'image_encoder2.model.stages.3.0.convs.0.norm.weight', 'image_encoder2.model.stages.3.0.convs.0.norm.bias', 'image_encoder2.model.stages.3.0.convs.0.all_modules.0.weight', 'image_encoder2.model.stages.3.0.convs.0.all_modules.0.bias', 'image_encoder2.model.stages.3.0.convs.0.all_modules.1.weight', 'image_encoder2.model.stages.3.0.convs.0.all_modules.1.bias', 'image_encoder2.model.stages.3.0.convs.1.conv.weight', 'image_encoder2.model.stages.3.0.convs.1.conv.bias', 'image_encoder2.model.stages.3.0.convs.1.norm.weight', 'image_encoder2.model.stages.3.0.convs.1.norm.bias', 'image_encoder2.model.stages.3.0.convs.1.all_modules.0.weight', 'image_encoder2.model.stages.3.0.convs.1.all_modules.0.bias', 'image_encoder2.model.stages.3.0.convs.1.all_modules.1.weight', 'image_encoder2.model.stages.3.0.convs.1.all_modules.1.bias', 'image_encoder2.model.stages.4.0.convs.0.conv.weight', 'image_encoder2.model.stages.4.0.convs.0.conv.bias', 'image_encoder2.model.stages.4.0.convs.0.norm.weight', 'image_encoder2.model.stages.4.0.convs.0.norm.bias', 'image_encoder2.model.stages.4.0.convs.0.all_modules.0.weight', 'image_encoder2.model.stages.4.0.convs.0.all_modules.0.bias', 'image_encoder2.model.stages.4.0.convs.0.all_modules.1.weight', 'image_encoder2.model.stages.4.0.convs.0.all_modules.1.bias', 'image_encoder2.model.stages.4.0.convs.1.conv.weight', 'image_encoder2.model.stages.4.0.convs.1.conv.bias', 'image_encoder2.model.stages.4.0.convs.1.norm.weight', 'image_encoder2.model.stages.4.0.convs.1.norm.bias', 'image_encoder2.model.stages.4.0.convs.1.all_modules.0.weight', 'image_encoder2.model.stages.4.0.convs.1.all_modules.0.bias', 'image_encoder2.model.stages.4.0.convs.1.all_modules.1.weight', 'image_encoder2.model.stages.4.0.convs.1.all_modules.1.bias', 'image_encoder2.model.stages.5.0.convs.0.conv.weight', 'image_encoder2.model.stages.5.0.convs.0.conv.bias', 'image_encoder2.model.stages.5.0.convs.0.norm.weight', 'image_encoder2.model.stages.5.0.convs.0.norm.bias', 'image_encoder2.model.stages.5.0.convs.0.all_modules.0.weight', 'image_encoder2.model.stages.5.0.convs.0.all_modules.0.bias', 'image_encoder2.model.stages.5.0.convs.0.all_modules.1.weight', 'image_encoder2.model.stages.5.0.convs.0.all_modules.1.bias', 'image_encoder2.model.stages.5.0.convs.1.conv.weight', 'image_encoder2.model.stages.5.0.convs.1.conv.bias', 'image_encoder2.model.stages.5.0.convs.1.norm.weight', 'image_encoder2.model.stages.5.0.convs.1.norm.bias', 'image_encoder2.model.stages.5.0.convs.1.all_modules.0.weight', 'image_encoder2.model.stages.5.0.convs.1.all_modules.0.bias', 'image_encoder2.model.stages.5.0.convs.1.all_modules.1.weight', 'image_encoder2.model.stages.5.0.convs.1.all_modules.1.bias', 'image_projection2.projection.weight', 'image_projection2.projection.bias', 'image_projection2.fc.weight', 'image_projection2.fc.bias', 'image_projection2.layer_norm.weight', 'image_projection2.layer_norm.bias'])\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('logs/shared_projector_shared_encoder/best.pt', map_location='cpu')\n",
    "print(checkpoint.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "609ba81e-5f4e-48a0-8be4-4007d3f7c652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(checkpoint['image_encoder1.model.stages.0.0.convs.0.conv.weight'], checkpoint['image_encoder2.model.stages.0.0.convs.0.conv.weight']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07cdbc8c-4f1d-4027-af6f-cb4e6d7ed84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "from nnunetv2.run.run_training import get_trainer_from_args\n",
    "from nnunetv2.run.load_pretrained_weights import load_pretrained_weights\n",
    "import config as CFG\n",
    "\n",
    "trainer = get_trainer_from_args(\n",
    "    dataset_name_or_id=CFG.nnUNet['dataset_name_or_id'],\n",
    "    configuration=CFG.nnUNet['configuration'],\n",
    "    fold=CFG.nnUNet['fold'],\n",
    "    trainer_name=CFG.nnUNet['trainer_name'],\n",
    "    plans_identifier=CFG.nnUNet['plans_identifier'],\n",
    "    device=torch.device('cpu'))\n",
    "trainer.initialize()\n",
    "\n",
    "# trainer.load_checkpoint(CFG.nnUNet['checkpoint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fc4b767-3a58-4cdb-bdc5-b4bb3ec0ed89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "encoder_checkpoint = OrderedDict()\n",
    "for k, v in checkpoint.items():\n",
    "    if 'image_encoder1.' in k:\n",
    "        encoder_checkpoint[k.replace('image_encoder1.model.', '')] = v\n",
    "    \n",
    "# print(encoder_checkpoint)\n",
    "load_success = trainer.network.encoder.load_state_dict(encoder_checkpoint)\n",
    "print(load_success)\n",
    "trainer.save_checkpoint('clip_pretrained_nnUNet.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-jepa]",
   "language": "python",
   "name": "conda-env-.conda-jepa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
