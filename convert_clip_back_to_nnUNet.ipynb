{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eae71a67-a21d-4d71-ba6b-dcc8990b0673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from CLIP import CLIPModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e0f1d9f-26fe-4a3b-befe-d01a91441fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('logs/shared_projector_shared_encoder/best.pt', map_location='cpu')\n",
    "checkpoint = torch.load('logs/CLIP_PreIVL_PostStent/best.pt', map_location='cpu')\n",
    "# print(checkpoint.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "609ba81e-5f4e-48a0-8be4-4007d3f7c652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(checkpoint['image_encoder1.model.stages.0.0.convs.0.conv.weight'], checkpoint['image_encoder2.model.stages.0.0.convs.0.conv.weight']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07cdbc8c-4f1d-4027-af6f-cb4e6d7ed84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "from nnunetv2.run.run_training import get_trainer_from_args\n",
    "from nnunetv2.run.load_pretrained_weights import load_pretrained_weights\n",
    "import config as CFG\n",
    "\n",
    "trainer = get_trainer_from_args(\n",
    "    dataset_name_or_id=CFG.nnUNet['dataset_name_or_id'],\n",
    "    configuration=CFG.nnUNet['configuration'],\n",
    "    fold=CFG.nnUNet['fold'],\n",
    "    trainer_name=CFG.nnUNet['trainer_name'],\n",
    "    plans_identifier=CFG.nnUNet['plans_identifier'],\n",
    "    device=torch.device('cpu'))\n",
    "trainer.initialize()\n",
    "\n",
    "# trainer.load_checkpoint(CFG.nnUNet['checkpoint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fc4b767-3a58-4cdb-bdc5-b4bb3ec0ed89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "encoder_checkpoint = OrderedDict()\n",
    "for k, v in checkpoint.items():\n",
    "    if 'image_encoder1.' in k:\n",
    "        encoder_checkpoint[k.replace('image_encoder1.model.', '')] = v\n",
    "    \n",
    "# print(encoder_checkpoint)\n",
    "load_success = trainer.network.encoder.load_state_dict(encoder_checkpoint)\n",
    "print(load_success)\n",
    "# trainer.save_checkpoint('clip_pretrained_nnUNet.pt')\n",
    "trainer.save_checkpoint('clip_preivl_poststent_pretrained_nnUNet.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c6f73b2-73a7-4702-9ba0-4181fa2f3441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "################### Loading pretrained weights from file  clip_pretrained_nnUNet.pt ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "trainer = get_trainer_from_args(\n",
    "    dataset_name_or_id=CFG.nnUNet['dataset_name_or_id'],\n",
    "    configuration=CFG.nnUNet['configuration'],\n",
    "    fold=CFG.nnUNet['fold'],\n",
    "    trainer_name=CFG.nnUNet['trainer_name'],\n",
    "    plans_identifier=CFG.nnUNet['plans_identifier'],\n",
    "    device=torch.device('cpu'))\n",
    "trainer.initialize()\n",
    "\n",
    "load_pretrained_weights(trainer.network, 'clip_pretrained_nnUNet.pt')\n",
    "for kv_nnUNet, kv_CLIP in zip(trainer.network.encoder.state_dict().items(), encoder_checkpoint.items()):\n",
    "    assert torch.eq(kv_nnUNet[1], kv_CLIP[1]).all(), f\"{kv_nnUNet[0]} and {kv_CLIP[0]} are not equal\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f848255b-347c-4f41-b1ae-d2697ea46471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "stages.0.0.convs.0.conv.weight and stages.0.0.convs.0.conv.weight are equal",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m trainer\u001b[38;5;241m.\u001b[39minitialize()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m kv_nnUNet, kv_CLIP \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(trainer\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mstate_dict()\u001b[38;5;241m.\u001b[39mitems(), encoder_checkpoint\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39meq(kv_nnUNet[\u001b[38;5;241m1\u001b[39m], kv_CLIP[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39many(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkv_nnUNet[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkv_CLIP[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are equal\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: stages.0.0.convs.0.conv.weight and stages.0.0.convs.0.conv.weight are equal"
     ]
    }
   ],
   "source": [
    "trainer = get_trainer_from_args(\n",
    "    dataset_name_or_id=CFG.nnUNet['dataset_name_or_id'],\n",
    "    configuration=CFG.nnUNet['configuration'],\n",
    "    fold=CFG.nnUNet['fold'],\n",
    "    trainer_name=CFG.nnUNet['trainer_name'],\n",
    "    plans_identifier=CFG.nnUNet['plans_identifier'],\n",
    "    device=torch.device('cpu'))\n",
    "trainer.initialize()\n",
    "\n",
    "\n",
    "inequal_keys = []\n",
    "for kv_nnUNet, kv_CLIP in zip(trainer.network.encoder.state_dict().items(), encoder_checkpoint.items()):\n",
    "    if not torch.eq(kv_nnUNet[1], kv_CLIP[1]).any()\n",
    "        inequal_keys.append(kv_nnUNet[0])\n",
    "\n",
    "print(inequal_keys)\n",
    "assert len(inequal_keys) > 0, 'all the keys are equal'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a1a4c0f-19cb-464d-9fa9-c93320edae3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(encoder_checkpoint['stages.0.0.convs.0.conv.weight'], trainer.network.encoder.state_dict()['stages.0.0.convs.0.conv.weight']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04df8473-940f-4908-bdbf-d5064556d88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from main import make_train_valid_dfs, build_loaders, valid_epoch\n",
    "import config as CFG\n",
    "\n",
    "\n",
    "CFG.experiment_name=\"CLIP_debug\"\n",
    "\n",
    "train_dataframe, valid_dataframe = make_train_valid_dfs()\n",
    "valid_dl = build_loaders(dataframe=valid_dataframe, mode='valid')\n",
    "\n",
    "model = CLIPModel()\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac085a4a-8983-41c6-b1f2-6b127dc01dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torchio/visualization.py:159: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "  0%|          | 0/15 [00:47<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torchio as tio\n",
    "\n",
    "model.eval()\n",
    "\n",
    "tqdm_object = tqdm(valid_dl, total=len(valid_dl))\n",
    "for iteration, batch in enumerate(tqdm_object):\n",
    "    device = CFG.device\n",
    "    batch['image1'] = batch['image1'][tio.DATA].permute(0, 1, 4, 3, 2).to(device)\n",
    "    batch['image2'] = batch['image2'][tio.DATA].permute(0, 1, 4, 3, 2).to(device)\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        visualize = iteration % CFG.visualize_every == 0\n",
    "        loss = model(batch, mode=\"valid\", visualize=visualize)\n",
    "\n",
    "    count = batch[\"image1\"].size(0)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61870ce-a6b6-4a3c-925c-9e37c01f31d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-jepa]",
   "language": "python",
   "name": "conda-env-.conda-jepa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
