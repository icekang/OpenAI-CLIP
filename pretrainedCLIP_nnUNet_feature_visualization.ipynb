{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f2bde3c-e52f-4060-9c9d-b7d2b694511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnunetv2.run.run_training import get_trainer_from_args\n",
    "from nnunetv2.run.load_pretrained_weights import load_pretrained_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "494cd46a-8b0a-4e46-b3eb-c6879c123403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import config as CFG\n",
    "import torch\n",
    "from main import make_train_valid_dfs, build_loaders, valid_epoch\n",
    "import torchio as tio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbd80e40-a798-451e-b292-b5f2eeca3b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/ModelGenesisOutputs/ModelGenesisNNUNetPretrainingV2_noNorm_correct_orientation/Converted_nnUNet_Genesis_OCT_Best.pt ###################\n"
     ]
    }
   ],
   "source": [
    "CFG.experiment_name=\"CLIP_debug_feature_visualization\"\n",
    "CFG.nnUNet['configuration'] = '3d_32x512x512_b2'\n",
    "CFG.nnUNet['dataset_name_or_id'] = '302'\n",
    "CFG.nnUNet['fold'] = '2'\n",
    "\n",
    "trainer = get_trainer_from_args(\n",
    "    dataset_name_or_id=CFG.nnUNet['dataset_name_or_id'],\n",
    "    configuration=CFG.nnUNet['configuration'],\n",
    "    fold=CFG.nnUNet['fold'],\n",
    "    trainer_name=CFG.nnUNet['trainer_name'],\n",
    "    plans_identifier=CFG.nnUNet['plans_identifier'],\n",
    "    device=torch.device('cpu'))\n",
    "trainer.initialize()\n",
    "# load_pretrained_weights(trainer.network, '/home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_32x512x512_b2/fold_2_pretrained_CLIP/checkpoint_best.pth')\n",
    "# load_pretrained_weights(trainer.network, '/home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt')\n",
    "load_pretrained_weights(trainer.network, '/home/gridsan/nchutisilp/datasets/ModelGenesisOutputs/ModelGenesisNNUNetPretrainingV2_noNorm_correct_orientation/Converted_nnUNet_Genesis_OCT_Best.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48916107-4192-4fe5-8416-5ce66dcfdbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataframe, valid_dataframe = make_train_valid_dfs()\n",
    "# valid_dl = build_loaders(dataframe=valid_dataframe, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0b31ca-a5e1-4959-890c-3904b210e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ef22542-1e61-44f9-ae59-2665a4e454ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = trainer.network.to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14743105-e10d-4286-a88f-d5466884b3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in valid_dl:\n",
    "#     device = CFG.device\n",
    "#     batch['image1'] = batch['image1'][tio.DATA].permute(0, 1, 4, 3, 2).to(device)\n",
    "#     batch['image2'] = batch['image2'][tio.DATA].permute(0, 1, 4, 3, 2).to(device)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "135b7d42-cf8c-4043-8212-03584ed62685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(batch, 'validation_batch.pth')\n",
    "batch = torch.load('validation_batch.pth', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43660471-4641-4ee0-b6fb-e6dee4c5dcd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 32, 512, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['image1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6987df1d-b237-46dd-97e4-cbf926bbffb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.network.eval()\n",
    "with torch.no_grad():\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        output = trainer.network.encoder(batch['image1'][0:1])\n",
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06d30c6e-2821-4576-a4fe-5b4fb0aff3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# %%\n",
    "def get_robust_pca(features: torch.Tensor, m: float = 2, remove_first_component=False):\n",
    "    # features: (N, C)\n",
    "    # m: a hyperparam controlling how many std dev outside for outliers\n",
    "    assert len(features.shape) == 2, \"features should be (N, C)\"\n",
    "    reduction_mat = torch.pca_lowrank(features, q=3, niter=20)[2]\n",
    "    colors = features @ reduction_mat\n",
    "    if remove_first_component:\n",
    "        colors_min = colors.min(dim=0).values\n",
    "        colors_max = colors.max(dim=0).values\n",
    "        tmp_colors = (colors - colors_min) / (colors_max - colors_min)\n",
    "        fg_mask = tmp_colors[..., 0] < 0.2\n",
    "        reduction_mat = torch.pca_lowrank(features[fg_mask], q=3, niter=20)[2]\n",
    "        colors = features @ reduction_mat\n",
    "    else:\n",
    "        fg_mask = torch.ones_like(colors[:, 0]).bool()\n",
    "    d = torch.abs(colors[fg_mask] - torch.median(colors[fg_mask], dim=0).values)\n",
    "    mdev = torch.median(d, dim=0).values\n",
    "    s = d / mdev\n",
    "    try:\n",
    "        rins = colors[fg_mask][s[:, 0] < m, 0]\n",
    "        gins = colors[fg_mask][s[:, 1] < m, 1]\n",
    "        bins = colors[fg_mask][s[:, 2] < m, 2]\n",
    "        rgb_min = torch.tensor([rins.min(), gins.min(), bins.min()])\n",
    "        rgb_max = torch.tensor([rins.max(), gins.max(), bins.max()])\n",
    "    except:\n",
    "        rins = colors\n",
    "        gins = colors\n",
    "        bins = colors\n",
    "        rgb_min = torch.tensor([rins.min(), gins.min(), bins.min()])\n",
    "        rgb_max = torch.tensor([rins.max(), gins.max(), bins.max()])\n",
    "\n",
    "    return reduction_mat, rgb_min.to(reduction_mat), rgb_max.to(reduction_mat)\n",
    "\n",
    "\n",
    "def get_pca_map_whole_volume(\n",
    "    feature_map: torch.Tensor,\n",
    "    img_size,\n",
    "    interpolation=\"bicubic\",\n",
    "    return_pca_stats=False,\n",
    "    pca_stats=None,\n",
    "    remove_first_component=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    feature_map: (num_frames, h, w, C) is the feature map of a single image.\n",
    "    \"\"\"\n",
    "    # print(feature_map.shape)\n",
    "    if feature_map.shape[0] != 1:\n",
    "        # make it (1, num_frames, h, w, C)\n",
    "        feature_map = feature_map[None]\n",
    "    if pca_stats is None:\n",
    "        reduct_mat, color_min, color_max = get_robust_pca(\n",
    "            feature_map.reshape(-1, feature_map.shape[-1]),\n",
    "            remove_first_component=remove_first_component,\n",
    "        )\n",
    "    else:\n",
    "        reduct_mat, color_min, color_max = pca_stats\n",
    "    pca_color = feature_map @ reduct_mat\n",
    "    pca_color = (pca_color - color_min) / (color_max - color_min)\n",
    "    pca_color = pca_color.clamp(0, 1)\n",
    "    resized_pca_colors = []\n",
    "    for i in range(pca_color.shape[1]):\n",
    "        resized_pca_color = F.interpolate(\n",
    "            pca_color[:, i, :, :, :].permute(0, 3, 1, 2),\n",
    "            size=img_size,\n",
    "            mode=interpolation,\n",
    "        ).permute(0, 2, 3, 1)\n",
    "        resized_pca_colors.append(resized_pca_color.cpu().numpy().squeeze(0))\n",
    "    pca_color = np.stack(resized_pca_colors, axis=0)\n",
    "    if return_pca_stats:\n",
    "        return pca_color, (reduct_mat, color_min, color_max)\n",
    "    return pca_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc561f2-a711-4c1c-8fec-2d0854fcad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in output:\n",
    "    print(feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9546c170-ddac-4ec0-8afe-18d7357a8d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "print(batch['image1'][0].shape)\n",
    "\n",
    "step = 4\n",
    "fig, axes = plt.subplots(nrows=1, ncols=batch['image1'][0].shape[1] // step, figsize=(40, 40))\n",
    "for i_col in range(0, batch['image1'][0].shape[1], step):\n",
    "    axes[i_col//step].imshow(batch['image1'][0][0, i_col], cmap='gray')\n",
    "fig.savefig('original_image_for_feature_visualization.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27473e4b-77e9-4b48-b88f-2808053f97aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "CFG.experiment_name=\"CLIP_debug_feature_visualization\"\n",
    "CFG.nnUNet['configuration'] = '3d_32x512x512_b2'\n",
    "CFG.nnUNet['dataset_name_or_id'] = '302'\n",
    "CFG.nnUNet['fold'] = '2'\n",
    "\n",
    "trainer = get_trainer_from_args(\n",
    "    dataset_name_or_id=CFG.nnUNet['dataset_name_or_id'],\n",
    "    configuration=CFG.nnUNet['configuration'],\n",
    "    fold=CFG.nnUNet['fold'],\n",
    "    trainer_name=CFG.nnUNet['trainer_name'],\n",
    "    plans_identifier=CFG.nnUNet['plans_identifier'],\n",
    "    device=torch.device('cpu'))\n",
    "trainer.initialize()\n",
    "load_pretrained_weights(trainer.network, '/home/gridsan/nchutisilp/datasets/ModelGenesisOutputs/ModelGenesisNNUNetPretrainingV2_noNorm_correct_orientation/Converted_nnUNet_Genesis_OCT_Best.pt')\n",
    "\n",
    "_ = trainer.network.to(CFG.device)\n",
    "\n",
    "for batch_index in range(0,16):\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            output = trainer.network.encoder(batch['image1'][batch_index:batch_index + 1])\n",
    "\n",
    "    for feature_index in range(len(output)):\n",
    "        feature = output[feature_index]\n",
    "        feature = get_pca_map_whole_volume(feature.permute(0, 2, 3, 4, 1), feature.shape[-2:], remove_first_component=True)\n",
    "\n",
    "        step = max(1, feature.shape[0] // 4)\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=feature.shape[0] // step, figsize=(35, 10))\n",
    "        for i_col in range(0, feature.shape[0], step):\n",
    "            axes[i_col//step].imshow(feature[i_col])\n",
    "\n",
    "        fig.savefig(f'genesis_feature_map_batch{batch_index}_feature{feature_index}.png')        \n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e2ede0-e47b-4e7d-84f0-e5f6ed7906b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "CFG.experiment_name=\"CLIP_debug_feature_visualization\"\n",
    "CFG.nnUNet['configuration'] = '3d_32x512x512_b2'\n",
    "CFG.nnUNet['dataset_name_or_id'] = '302'\n",
    "CFG.nnUNet['fold'] = '2'\n",
    "\n",
    "feature_index = 5\n",
    "\n",
    "\n",
    "trainer = get_trainer_from_args(\n",
    "    dataset_name_or_id=CFG.nnUNet['dataset_name_or_id'],\n",
    "    configuration=CFG.nnUNet['configuration'],\n",
    "    fold=CFG.nnUNet['fold'],\n",
    "    trainer_name=CFG.nnUNet['trainer_name'],\n",
    "    plans_identifier=CFG.nnUNet['plans_identifier'],\n",
    "    device=torch.device('cpu'))\n",
    "trainer.initialize()\n",
    "load_pretrained_weights(trainer.network, '/home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt')\n",
    "trainer.network.eval()\n",
    "with torch.no_grad():\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        output = trainer.network.encoder(batch['image1'][0:1])\n",
    "\n",
    "\n",
    "\n",
    "feature = output[feature_index]\n",
    "feature = get_pca_map_whole_volume(feature.permute(0, 2, 3, 4, 1), (512, 512))\n",
    "\n",
    "step = 1\n",
    "fig, axes = plt.subplots(nrows=1, ncols=feature.shape[0] // step, figsize=(40, 40))\n",
    "for i_col in range(0, feature.shape[0], step):\n",
    "    axes[i_col//step].imshow(feature[i_col])\n",
    "\n",
    "\n",
    "trainer = get_trainer_from_args(\n",
    "    dataset_name_or_id=CFG.nnUNet['dataset_name_or_id'],\n",
    "    configuration=CFG.nnUNet['configuration'],\n",
    "    fold=CFG.nnUNet['fold'],\n",
    "    trainer_name=CFG.nnUNet['trainer_name'],\n",
    "    plans_identifier=CFG.nnUNet['plans_identifier'],\n",
    "    device=torch.device('cpu'))\n",
    "trainer.initialize()\n",
    "trainer.network.eval()\n",
    "with torch.no_grad():\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        output = trainer.network.encoder(batch['image1'][0:1])\n",
    "\n",
    "feature = output[feature_index]\n",
    "feature = get_pca_map_whole_volume(feature.permute(0, 2, 3, 4, 1), (512, 512))\n",
    "\n",
    "step = 1\n",
    "fig, axes = plt.subplots(nrows=1, ncols=feature.shape[0] // step, figsize=(40, 40))\n",
    "for i_col in range(0, feature.shape[0], step):\n",
    "    axes[i_col//step].imshow(feature[i_col])\n",
    "# fig.savefig('default_feature_map_0.pdf')\n",
    "# fig.savefig('clip_feature_map_0.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0be8958-ead4-4532-ac9e-f97a5244841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CLIP import CLIPModel\n",
    "\n",
    "CFG.experiment_name = 'CLIP_debug_nnUnet'\n",
    "model = CLIPModel()\n",
    "# model.image_encoder1.load_state_dict(torch)\n",
    "CLIP_batch = dict()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        # CLIP_batch['image1'] = batch['image1'][tio.DATA].permute(0, 1, 4, 3, 2).to(device)\n",
    "        # CLIP_batch['image2'] = batch['image2'][tio.DATA].permute(0, 1, 4, 3, 2).to(device)\n",
    "        loss = model(batch, mode=\"valid\", visualize=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dad0acc1-6ef0-4a82-bd5b-450fe7899c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "from CLIP import CLIPModel\n",
    "\n",
    "CFG.experiment_name = 'CLIP_debug_nnUnet'\n",
    "model2 = CLIPModel(shared_projector=True, shared_encoder=True)\n",
    "\n",
    "model1 = CLIPModel()\n",
    "model1.load_state_dict(torch.load('logs/shared_projector_shared_encoder/best.pt', map_location='cpu'))# model.image_encoder1.load_state_dict(torch)\n",
    "\n",
    "#load only projector weight to CLIP\n",
    "# model2.image_projection2 = model1.image_projection2\n",
    "# model2.image_projection1 = model1.image_projection1\n",
    "\n",
    "model = model1\n",
    "CLIP_batch = dict()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        # CLIP_batch['image1'] = batch['image1'][tio.DATA].permute(0, 1, 4, 3, 2).to(device)\n",
    "        # CLIP_batch['image2'] = batch['image2'][tio.DATA].permute(0, 1, 4, 3, 2).to(device)\n",
    "        image1 = batch[\"image1\"]\n",
    "        image2 = batch[\"image2\"]\n",
    "\n",
    "        # Check for NaN values\n",
    "        masks = torch.isnan(image1) | torch.isnan(image2)\n",
    "        masks = ~masks.any(dim=(1, 2, 3, 4))\n",
    "        image1 = image1[masks]\n",
    "        image2 = image2[masks]\n",
    "\n",
    "        # Getting Image and Text Features\n",
    "        visual_features1 = model.image_encoder1.model(image1)\n",
    "        visual_features2 = model.image_encoder2.model(image2)\n",
    "        image_features1 = model.image_encoder1(image1)\n",
    "        image_features2 = model.image_encoder2(image2)\n",
    "\n",
    "        # Getting Image and Text Embeddings (with same dimension)\n",
    "        image_embeddings1 = model.image_projection1(image_features1)\n",
    "        image_embeddings2 = model.image_projection2(image_features2)\n",
    "        # loss = model(batch, mode=\"valid\", visualize=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29519f9a-47bc-4cde-b552-9ac2027c57aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# torch.save(image_embeddings1, 'image_embeddings1.pth')\n",
    "# torch.save(image_embeddings2, 'image_embeddings2.pth')\n",
    "\n",
    "image_embedding_all = torch.cat([image_embeddings1, image_embeddings2], dim=0)\n",
    "reduction_mat = torch.pca_lowrank(image_embedding_all, q=2, niter=20)[2]\n",
    "pca_features_all = image_embedding_all @ reduction_mat\n",
    "\n",
    "# print(reduction_mat.shape, colors.shape)\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import colormaps\n",
    "\n",
    "colors = colormaps['tab20']\n",
    "total = image_embeddings1.shape[0]\n",
    "np.random.seed(0)\n",
    "\n",
    "for i in range(total):\n",
    "    pair = pca_features_all[[i, i + total]]\n",
    "    plt.scatter(pair[:, 0], pair[:, 1], color=colors(i))\n",
    "plt.xlabel(\"PCA2\")\n",
    "plt.ylabel(\"PCA1\")\n",
    "plt.figure(figsize=(10, 10), dpi=80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe105b-bfd3-47b6-bddd-9a56556ee931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# back\n",
    "clip_real_state_dict = torch.load('logs/shared_projector_shared_encoder/best.pt', map_location='cpu')\n",
    "clip_nnunet_state_dict = torch.load('clip_pretrained_nnUNet.pt', map_location='cpu')\n",
    "for w in clip_nnunet_state_dict['network_weights']:\n",
    "    print(torch.eq(clip_real_state_dict[w.replace('encoder', 'image_encoder1.model')], clip_nnunet_state_dict['network_weights'][w]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "45871868-5773-4742-8cbd-f0e3c36ce0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for batch_index in range(0,16):\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            output1 = model1.image_encoder1.model(batch['image1'][batch_index:batch_index + 1])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            output2 = model2.image_encoder1.model(batch['image1'][batch_index:batch_index + 1])\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(35, 10))\n",
    "    step = batch['image1'][batch_index].shape[1] // 4\n",
    "    for i_col in range(0, batch['image1'][batch_index].shape[1], step):\n",
    "        axes[i_col//step].imshow(batch['image1'][batch_index][0][i_col], cmap='gray')\n",
    "    fig.savefig(f'image1_batch{batch_index}.png')\n",
    "    plt.close()\n",
    "    for feature_index in range(len(output2)):\n",
    "        feature = output1[feature_index]\n",
    "        feature = get_pca_map_whole_volume(feature.permute(0, 2, 3, 4, 1), feature.shape[-2:], remove_first_component=True)\n",
    "\n",
    "        step = max(1, feature.shape[0] // 4)\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=feature.shape[0] // step, figsize=(35, 10))\n",
    "        for i_col in range(0, feature.shape[0], step):\n",
    "            axes[i_col//step].imshow(feature[i_col])\n",
    "\n",
    "        fig.savefig(f'clip_feature_map_batch{batch_index}_feature{feature_index}.png')        \n",
    "        plt.close()\n",
    "        feature = output2[feature_index]\n",
    "        feature = get_pca_map_whole_volume(feature.permute(0, 2, 3, 4, 1), feature.shape[-2:], remove_first_component=True)\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=feature.shape[0] // step, figsize=(35, 10))\n",
    "        for i_col in range(0, feature.shape[0], step):\n",
    "            axes[i_col//step].imshow(feature[i_col])\n",
    "\n",
    "        fig.savefig(f'default_feature_map_batch{batch_index}_feature{feature_index}.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bba4c2-1b29-4ceb-b538-229ca0eda069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-jepa]",
   "language": "python",
   "name": "conda-env-.conda-jepa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
